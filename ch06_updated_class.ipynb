{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 데이터 로딩과 저장, 파일 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"figure\", figsize=(10, 6))\n",
    "pd.options.display.max_colwidth = 75\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_rows = 10\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.rcParams[\"font.family\"] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. csv 텍스트 데이터\n",
    "## 1.1 csv파일 읽기\n",
    "* 판다스에서 CSV 파일을 읽기: pd.read_csv(path, encoding = 'utf-8') 함수를 사용\n",
    "    * path 인수 : 파일시스템에서의 위치, URL, 파일객체를 나타내는 문자열\n",
    "    * encoding 인수 - 유니코드 인코딩 종류을 지정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) header(컬럼명)가 있는 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!cat examples/ex1.csv\n",
    "\n",
    "df = pd.read_csv(\"examples/ex1.csv\") # 데이터프레임으로 반환\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2) header가 없는 파일 : 컬럼을 지정\n",
    "        * names 인수 - 열 이름 리스트\n",
    "        * header 인수 - 헤더가 없을 경우 None으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!cat examples/ex2.csv\n",
    "names = [\"a\", \"b\", \"c\", \"d\", \"message\"]\n",
    "data = pd.read_csv(\"examples/ex2.csv\", names=names)\n",
    "data['message'] # Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3) 계층적 색인 지정하기\n",
    "        * index_col 인수 - 색인으로 사용할 열 번호나 이름, 계층적 색인을 지정할 경우 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat examples/csv_mindex.csv\n",
    "parsed = pd.read_csv(\"examples/csv_mindex.csv\",\n",
    "                     index_col=[\"key1\", \"key2\"])\n",
    "parsed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4) 여러 개의 공백으로 필드를 구분한 파일 \n",
    "        * sep 인수 - 필드을 구분하기 위해 사용할 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat examples/ex3.txt\n",
    "\n",
    "result = pd.read_csv(\"examples/ex3.txt\",sep=\"\\s+\") #\\s+ -> 스페이스바가 0개 이상\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5) 주석을 갖고 있는 파일\n",
    "        * skiprows 인수 - 파일의 시작부터 무시할 행 수 또는 무시할 행 번호가 담긴 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat examples/ex4.csv\n",
    "pd.read_csv(\"examples/ex4.csv\", skiprows=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6) **누락된 값이 있는 파일 : 비어 있는 문자열, NA, NULL을 NaN으로 출력** \n",
    "        * na_values 인수 - NA 값으로 처리할 값들의 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat examples/ex5.csv\n",
    "\n",
    "result = pd.read_csv(\"examples/ex5.csv\")\n",
    "result\n",
    "\n",
    "# 1) 누락된 값을 확인 : isna(), isnull(): 결측치 True, isnot(), notNa(): 결측치가 아닌 것 True\n",
    "pd.isna(result)\n",
    "\n",
    "# 2) na_values -> 컬럼별 문자열 집합을 받아서 누락된 값 처리\n",
    "sentinels = {\"message\": [\"foo\", \"NA\"], \"something\": [\"two\"]}\n",
    "result = pd.read_csv(\"examples/ex5.csv\", na_values=sentinels)\n",
    "result\n",
    "result.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7) 텍스트 파일 조금씩 \n",
    "        * nrows 인수 - 파일의 첫 일부만 읽어올 때 처음 몇 줄을 읽을 것인지 지정 \n",
    "        * chunksize 인수 - TextParser 객체에서 사용할 한 번에 읽을 파일의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pd.read_csv(\"examples/ex6.csv\")\n",
    "# result\n",
    "result = pd.read_csv(\"examples/ex6.csv\", nrows=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제]  ex6.csv 파일의 \"key\" 컬럼의 값들에 대한 빈도수을 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!cat examples/ex6.csv\n",
    "chunker = pd.read_csv(\"examples/ex6.csv\", chunksize=1000)\n",
    "chunker\n",
    "tot = pd.Series([], dtype='int64')\n",
    "\n",
    "# \"key\" 컬럼의 값들에 대한 누적의 합\n",
    "for piece in chunker:# 10번 반복 수행\n",
    "    # print(piece[\"key\"].value_counts())\n",
    "    tot = tot.add(piece[\"key\"].value_counts(), fill_value=0)\n",
    "\n",
    "tot\n",
    "#값들을 정렬\n",
    "tot = tot.sort_values(ascending=False)\n",
    "tot\n",
    "tot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시리즈 간 덧셈 연산\n",
    "from pandas import Series\n",
    "tot = Series([], dtype= 'int64')\n",
    "tot1 = Series([1,2,3])\n",
    "#tot + tot1\n",
    "# tot.add(tot1)\n",
    "# 누적의 합\n",
    "for i in range(3):\n",
    "    tot = tot.add(tot1, fill_value = 0)\n",
    "tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. csv 텍스트 파일 저장하기\n",
    "* 판다스에서 CSV 파일을 저장하기: df.to_csv(path, encoding = 'utf-8') 함수를 사용\n",
    "    * path 인수 : 파일시스템에서의 위치, URL, 파일객체를 나타내는 문자열\n",
    "    * encoding 인수 - 유니코드 인코딩 종류을 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일로 저장\n",
    "# 1. 데이터프레임 준비\n",
    "data = pd.read_csv(\"examples/ex5.csv\") # 원본에 대한 데이터프레임\n",
    "data\n",
    "data1 = data[['a','b','d']] # 컬럼 인덱싱 이용 데이터 추출\n",
    "data1\n",
    "# 2. 데이터프레임을 파일로 저장\n",
    "data1.to_csv(\"examples/out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"examples/out.csv\", sep=\"|\") # 구분자 사용\n",
    "data.to_csv(\"examples/out.csv\", na_rep = \"NULL\") #누락된 값을 원하는 값으로 지정\n",
    "data.to_csv(\"examples/out.csv\", index=False, header=False)\n",
    "data.to_csv(\"examples/out.csv\", index=False, columns=[\"a\", \"b\", \"c\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. JSON  데이터\n",
    "* JSON(JavaScript Object Notation)은 텍스트 데이터를 저장하고 전송하기 위한 일반적인 형식\n",
    "* 주로 웹에서 데이터를 전송하거나 저장하는 데 사용\n",
    "* JSON 파일은 일반적으로 텍스트 형식으로 작성되며, 사람과 기계 모두 이해하기 쉽다.\n",
    "* JSON 파일은 키-값 쌍의 모음으로 구성\n",
    "  * 키-값 쌍은 객체(object)라고도 하며, 중괄호 {} 로 묶는다.\n",
    "  * 배열(array)은 대괄호 [] 안에 값의 목록을 나열하여 정의."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 JSON 데이터 읽기/저장\n",
    "    * pandas의 to_json() -> 데이터프레임을 JSON 파일로 저장\n",
    "    * read_json() -> JSON 파일에서 데이터프레임을 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제]  pandas의 to_json() 및 read_json() 메서드를 사용하여 데이터프레임을 JSON 파일로 저장하고 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. JSON 파일로 저장할 데이터프레임 생성\n",
    "data = {\n",
    "    \"name\": [\"John\", \"Alice\", \"Bob\"],\n",
    "    \"age\": [30, 25, 35],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. 데이터프레임을 JSON 파일로 저장\n",
    "df.to_json(\"examples/data.json\")\n",
    "\n",
    "# 3. JSON 파일을 읽어와 데이터프레임으로 변환\n",
    "df_read = pd.read_json(\"examples/data.json\")\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nDataFrame from JSON file:\")\n",
    "print(df_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. HTML 데이터\n",
    "## 3.1 웹 스크래핑 - read_html()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* read_html() - html 문서 내 테이블 데이터 읽어 테이블 구조의 모든 데이터를 데이터프레임으로 저장해서 반환\n",
    "    * https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제] 미국 예금 보험 공사에서 부도 은행을 보여주는 HTML 문서의 테이블 데이터 읽어오기\n",
    "    * 연도별로 부도 은행의 개수을 계산하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#tables = pd.read_html(\"examples/fdic_failed_bank_list.html\") \n",
    "#1. 정적웹페이지 데이터 읽어오기\n",
    "tables = pd.read_html(\"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\")\n",
    "type(tables)\n",
    "len(tables)\n",
    "failures = tables[0] #인덱싱\n",
    "failures\n",
    "type(failures)\n",
    "failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. 연도별로 부도 은행의 개수 : DataFrame 가공\n",
    "# 1) 컬럼명 확인 /수정하기 /확인\n",
    "failures.columns\n",
    "failures.columns = ['BankNameBank', 'CityCity', 'StateSt', 'CertCert', 'AcquiringInstitutionAI', \\\n",
    "                    'ClosingDate', 'FundFund']\n",
    "failures.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) 데이터 변환 : 문자열 object -> datetime\n",
    "failures[\"ClosingDate\"] # 컬럼 인덱싱을 이용해서 시리즈로 추출\n",
    "close_timestamps = pd.to_datetime(failures[\"ClosingDate\"]) \n",
    "close_timestamps\n",
    "# 3) 년도별 데이터 개수 추출\n",
    "close_timestamps.dt.year\n",
    "close_timestamps.dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] : 네이버 주식 데이터을 스크래핑하여 csv파일로 저장하기 - read_html()\n",
    "* https://www.naver.com/ -> 증권 -> 국내증시 -> 시가총액\n",
    "    * https://finance.naver.com/sise/sise_market_sum.naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install html5lib  lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 1) 웹페이지 읽어오기 \n",
    "# 'euc-kr' 또는 'cp949'로 인코딩 설정하여 페이지 읽기 시도\n",
    "df_list = pd.read_html(\"https://finance.naver.com/sise/sise_market_sum.naver\",encoding='cp949')\n",
    "df_list\n",
    "type(df_list)\n",
    "df_list[1] # 인덱싱\n",
    "# 데이터프레임 확인\n",
    "for df in df_list:\n",
    "    print(\"\\n\",df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 데이터프레임 가공 : csv파일로 저장하기\n",
    "df = df_list[1]\n",
    "type(df)\n",
    "df.head(10)\n",
    "# (1) 데이터프레임 탐색 :  종목명 컬럼명으로 인덱싱 - NaN 처리\n",
    "df['종목명']\n",
    "# (2) 조건색인\n",
    "cond = df['종목명'].notnull() #불리언 시리즈\n",
    "data = df[cond] # 불리언색인을 이용하여 필터링\n",
    "data\n",
    "data.head(10)\n",
    "#3) 데이터프레임 저장\n",
    "data.to_csv(\"examples/Naver_kospi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 웹 스크래핑 - requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 웹 페이지 정보 가져오기 - requests 패키지 이용\n",
    "  * requests.get()\n",
    "  * requests.get().status_code\n",
    "  * requests.get().text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제] 패키지 requests을 활용하여 웹페이지 json 데이터을 데이터프레임으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "#1. 타겟 웹사이트 접속\n",
    "url = \"https://api.github.com/repos/pandas-dev/pandas/issues\"\n",
    "resp = requests.get(url)\n",
    "resp.status_code\n",
    "# 2. 웹페이지 콘텐츠 읽어오기\n",
    "resp.text # 문자열로 반환\n",
    "#pd.read_html(url)\n",
    "# 3. json 내용을 딕셔너리로 변환\n",
    "data = resp.json() # json의 내용을 딕셔너리 형태로 변환한 객체을 반환\n",
    "print(data[0].keys()) # 키 정보 출력\n",
    "# data[0]['number']\n",
    "\n",
    "#4. 딕셔너리를 DataFrame으로 생성하기\n",
    "issues = pd.DataFrame(data, columns=[\"number\", \"title\",\n",
    "                                      \"labels\", \"state\"])\n",
    "issues.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] 네이버 블로그의 키워드 관련 검색 결과 텍스트 데이터 가져오기 - requests 모듈 사용시 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://section.blog.naver.com/Search/Post.naver?pageNo=1&rangeType=ALL&orderBy=sim&keyword=%ED%8C%8C%EC%9D%B4%EC%8D%AC\"\n",
    "\n",
    "# params = {\n",
    "#     'pageNo' : 1,\n",
    "#     'rangeType' : 'ALL',\n",
    "#     'orderBy' : 'sim',\n",
    "#     'keyword' : '파이썬'\n",
    "# }\n",
    "\n",
    "# response = requests.get('https://section.blog.naver.com/Search/Post.naver?', params=params)\n",
    "response = requests.get(url)\n",
    "print(response.status_code) # HTTP GET 방식으로 URL 파라미터 값 전달\n",
    "print(response.url) # status_code 는 응답코드\n",
    "print(response.text) # text에는 HTML 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 웹 스크래핑 - beautifulsoup\n",
    "* 웹 페이지 정보 추출하기 - beautifulsoup 사용법 \n",
    "  * soup.select_one()\n",
    "  * soup.select()\n",
    "  * get_text()\n",
    "  * get_attribute()\n",
    "* 개발자 도구 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] 네이버 지식iN의 검색어 관련 검색 결과 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#타겟 웹사이트 지정\n",
    "url = 'https://kin.naver.com/search/list.naver?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'\n",
    "\n",
    "# 웹페이지 로딩\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:    \n",
    "    # beautifulsoup 객체로 변환\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 한개 선택하여 텍스트 가져오기\n",
    "    title = soup.select_one('#s_content > div.section > ul > li:nth-child(1) > dl > dt > a')\n",
    "    # print(title)\n",
    "    # print(title.get_text())\n",
    "    # print(title.get_attribute_list(\"href\"))\n",
    "\n",
    "    # 제목 여러 개 선택하여 텍스트 리스트 가져오기\n",
    "    # s_content > div.section > ul > li:nth-child(2) > dl > dt > a\n",
    "    # s_content > div.section > ul > li:nth-child(3) > dl > dt > a\n",
    "    titles = soup.select('#s_content > div.section > ul > li:nth-child(n) > dl > dt > a')\n",
    "    # print(titles)\n",
    "    for title in titles:\n",
    "        print(\"\\n\",title.get_text())\n",
    "        print(\"\\n\",title.get_attribute_list(\"href\"))\n",
    "else : \n",
    "    print(response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 웹 스크래핑 - Selenium을 이용하여 크롤러( 6주차)\n",
    "*  https://selenium-python.readthedocs.io/\n",
    "* selenium 사용법\n",
    "  1. 시스템의 OS 및 Chrome 브라우저의 버전 확인\n",
    "  2. Chrome 드라이버 다운로드 및 설치\n",
    "  3. 크롬 드라이버을 프로젝트 폴더에 저장 - 크롬 드라이버의 설치 경로\n",
    "     * from selenium import webdriver\n",
    "     * driver = webdriver.Chrome()\n",
    "* 동적으로 웹 페이지 정보 추출하기\n",
    "     * driver.get(\"웹주소\") \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제]Selenium을 이용하여 자동으로 타겟 웹 사이트 실행하기\n",
    "* 타겟 사이트 : https://www.google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "# 1. Chrome 브라우저 실행\n",
    "driver = webdriver.Chrome() # 크롬 드라이버 실행파일의 위치 - 작업디렉터리\n",
    "# 2. 타겟 웹페이지 로딩\n",
    "url = \"https://www.google.com\"\n",
    "driver.get(url)\n",
    "# 3. 타겟 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] Selenium을 활용하여 타겟 웹사이트 자동으로 열기/ 쿼리로 검색하기/ 검색 결과의 제목과 url을 자동으로 추출하여 저장하는 web crawler 만들기\n",
    "    * 사전 준비 작업 : 타겟 웹사이트을 구글 개발도구을 이용하여 탐색하기\n",
    "    * 타겟 사이트: https://www.google.com\n",
    "    * driver.find_element()\n",
    "    * send_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# 1. Chrome 브라우저 실행\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 2. 타겟 Google 홈페이지 열기\n",
    "url = \"https://www.google.com\"\n",
    "driver.get(url)\n",
    "\n",
    "# 3. 페이지가 완전히 로드될 때까지 2초 대기\n",
    "time.sleep(2)\n",
    "\n",
    "# 4. 검색 기능 - 검색창-> 검색어 입력 -> 엔터 키로 클릭 처리\n",
    "try:\n",
    "    # 4-1 검색 상자 요소 찾기\n",
    "    search_box = driver.find_element(By.NAME, \"q\") \n",
    "\n",
    "    # 4-2 검색어 입력\n",
    "    search_query = \"OpenAI\"\n",
    "    search_box.send_keys(search_query) \n",
    "\n",
    "    # 4-3 Enter 키를 눌러 검색 수행\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # 검색 결과가 로드될 때까지 5초 대기\n",
    "    time.sleep(5)\n",
    "\n",
    "    # 5. 검색에 관련된  결과 추출\n",
    "    search_results = driver.find_elements(By.CSS_SELECTOR, \"div.g\") \n",
    "        \n",
    "    # 5-1 첫 번째 제목과 링크 url 추출\n",
    "    title = search_results[0].find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "    url = search_results[0].find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "    print(\"제목:\", title)\n",
    "    print(\"URL:\", url)\n",
    "\n",
    "    # 5-2 검색 결과의 모든 제목과 링크 URL 인쇄\n",
    "    for result in search_results:\n",
    "        title = result.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "        url = result.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "        print(\"제목:\", title)\n",
    "        print(\"URL:\", url)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"오류가 발생:\", e)\n",
    "\n",
    "finally:\n",
    "    # 브라우저 창 닫기\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] Selenium을 이용하여 네이버 증권 웹페이지 열기/ 삼성전자 일별 시세 HTML가져오기/ 10페이지 내역 테이블 데이터 찾기/검색 결과를 데이터프레임으로 변환/저장하기\n",
    "* 타겟 사이트: https://finance.naver.com/item/sise.naver?code=005930* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           날짜       종가    전일비       시가       고가       저가         거래량\n",
      "1  2024.04.11  84100.0  500.0  83200.0  84700.0  82500.0  25328541.0\n",
      "2  2024.04.09  83600.0  900.0  84500.0  84900.0  83100.0  23725956.0\n",
      "3  2024.04.08  84500.0    0.0  85200.0  86000.0  84500.0  18953232.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
      "/var/folders/fk/f8wj7xqx1wzf8_4zjpr7hrth0000gn/T/ipykernel_48313/3820261275.py:20: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n"
     ]
    }
   ],
   "source": [
    "#1. 크롬 드라이버 실행\n",
    "driver= webdriver.Chrome()\n",
    "\n",
    "# 2페이지 로딩 시간\n",
    "time.sleep(2)\n",
    "\n",
    "# 3. 크롤링 사이트 : 삼성전자의 일별 시세 페이지 URL 찾기\n",
    "samsung_url =\"https://finance.naver.com/item/sise_day.naver?code=005930&page=\" #base url\n",
    "\n",
    "# 4. 첫 페이지부터 10 페이지까지 테이블 담기\n",
    "samsung_daily_kospi = [] # 리스트 선언\n",
    "for num in range(1,11):\n",
    "    full_url = samsung_url + str(num)\n",
    "    # 4.1 해당 페이지 요청\n",
    "    driver.get(full_url)\n",
    "        \n",
    "    try:\n",
    "        # 4.2 페이지의 HTML 가져오기\n",
    "        html = driver.page_source\n",
    "        df = pd.read_html(html) # 테이블 구조 -> 데이터프레임\n",
    "        # 4.3 결측치 처리 - 조건 색인을 이용한 불리언 인덱싱\n",
    "        ## 조건 색인 인덱싱\n",
    "        cond = df[0]['날짜'].notnull()\n",
    "        samsung_day = df[0][cond]\n",
    "        ## 리스트에 담기\n",
    "        samsung_daily_kospi.append(samsung_day)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Network Error\",e)\n",
    "    \n",
    "    # finally:\n",
    "    #     driver.quit()\n",
    "\n",
    " # 5. 데이터프레임 합치기\n",
    "samsung_daily_kospi_all = pd.concat(samsung_daily_kospi)\n",
    "samsung_daily_kospi_all.head(10)\n",
    "samsung_daily_kospi_all.to_csv(\"samsung_daily_kospi_2024_04_11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
